{
    "name": "LLMs and Multi-Agent Orchestration",
    "build": {
        "dockerfile": "Dockerfile",
        "context": "."
    },
    "customizations": {
        "vscode": {
            "settings": {
                "terminal.integrated.defaultProfile.linux": "zsh",
                "python.defaultInterpreterPath": "../.venv/bin/python",
                "terminal.integrated.stickyScroll.enabled": false
            },
            "extensions": [
                "ms-python.python",
                "ms-python.vscode-pylance",
                "charliermarsh.ruff",
                "openai.chatgpt",
                "docker.docker",
                "ms-azuretools.vscode-containers",
                "tamasfe.even-better-toml",
                "christian-kohler.path-intellisense"
            ]
        }
    },
    "features": {
        "ghcr.io/devcontainers/features/common-utils:2": {
            "installZsh": true,
            "installOhMyZsh": true,
            "configureZshAsDefaultShell": true
        },
        "ghcr.io/devcontainers-extra/features/zsh-plugins": {
            "plugins": "uv"
        },
        "ghcr.io/devcontainers/features/docker-in-docker": {}
    },
    "forwardPorts": [
        11434
    ],
    "postCreateCommand": {
        "installPython": "uv python install 3.13",
        "installReflexTool": "uv tool install reflex",
        "installSpecKit": "uv tool install specify-cli --from git+https://github.com/github/spec-kit.git",
        "setupNodeSymlink": "ln -sf /root/.bun/bin/bun /usr/bin/node",
        "installCodex": "bun install -g @openai/codex",
        "setupOllama": "nohup ollama serve > /tmp/ollama-serve.log 2>&1 & sleep 5"
    },
    "postStartCommand": "ollama pull llama3.2:latest && echo 'Development container ready!'",
    "remoteUser": "root"
}