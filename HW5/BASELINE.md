# BASELINE

This document doubles as the "short baseline report" required in
`ollama-rag-installation.md`. It captures the exact configuration,
metrics, and outstanding evidence for Sub‑experiment 1.

## System Information

- __Ollama version__: 0.12.6
- __OS__: Debian GNU/Linux 12 (bookworm)
- __CPU cores__: 10

## Pipeline Configuration (matches video)

- __Server__: `ollama serve` on `http://localhost:11434`
- __LLM__: `llama3.2` (local Ollama)
- __Embedding__: `nomic-embed-text` pulled via `ollama pull nomic-embed-text`
- __Chunker__: `RecursiveCharacterTextSplitter` with `chunk_size=1200`,
  `chunk_overlap=300`
- __Vector store__: Chroma persisted to `results/chroma_baseline`
- __Retriever__: `similarity_search` with `k=3`
- __Prompt__: "Answer the question based ONLY on the following context: ..."
- __Script__: `python experiments/experiment_1_baseline.py`

> Checklist reminder: capture terminal output of the matching
> `ollama pull/create/run` commands and store screenshots alongside the results folder.

## Metrics Summary

These values come from `results/experiment_1_baseline.json` (generated by the
current script). Re‑run the experiment after any code/config changes to refresh.

| Metric | Value |
| --- | --- |
| Document | `data/BOI.pdf` |
| Indexing time | `12.76s` |
| Number of chunks | `48` |
| Average response time (3 queries) | `50.87s` |
| Accuracy (auto check) | `Pending` — requires manual validation |

### Query-level snapshot

| Question | Response time | Accuracy note |
| --- | --- | --- |
| How to report BOI? | 45.97s | Pending manual confirmation |
| What is the document about? | 50.12s | Pending manual confirmation |
| Main points for business owners? | 56.51s | Pending manual confirmation |

Use `utils.metrics.print_metrics_summary` output plus manual review to update the
`is_accurate` flags inside the JSON file (or rerun the script after answering the
prompts). The new heuristic checker in `queries.evaluate_query_accuracy` provides
an initial guess, but you should confirm against the known answers.

## Evidence & Outstanding Items

1. __Video-specific answers__ — record short notes for the required questions
   (e.g., recommended embedding model, default port rationale).
2. __Terminal screenshots__ — include proofs of the exact `ollama pull/create/run`
   commands from the baseline run.
3. __Future sub-experiments__ — experiments 2–4 are still outstanding. When you
   run them, log their metrics in separate result files and update the comparison
   table described in the assignment.
4. __Manual accuracy review__ — mark each baseline response as Correct/Incorrect
   and commit the updated JSON so the baseline report satisfies the "answer
   quality" requirement.
